2022-09-03 16:40:12,662 (utils:129) INFO: the current machine is at 172.27.106.76
2022-09-03 16:40:12,663 (utils:131) INFO: the current dir is C:\Users\Public\FederatedScope4
2022-09-03 16:40:12,665 (utils:132) INFO: the output dir is exp\fedem_gin_on_cikmcup_lr0.25_lstep16_\sub_exp_20220903164012
2022-09-03 16:40:13,365 (cikm_cup:57) INFO: Loading CIKMCUP data from C:\Users\Public\FederatedScope4\data\CIKM22Competition.
2022-09-03 16:40:13,367 (cikm_cup:67) INFO: Loading CIKMCUP data for Client #1.
2022-09-03 16:40:13,609 (cikm_cup:67) INFO: Loading CIKMCUP data for Client #2.
2022-09-03 16:40:13,631 (cikm_cup:67) INFO: Loading CIKMCUP data for Client #3.
2022-09-03 16:40:13,941 (cikm_cup:67) INFO: Loading CIKMCUP data for Client #4.
2022-09-03 16:40:13,954 (cikm_cup:67) INFO: Loading CIKMCUP data for Client #5.
2022-09-03 16:40:13,978 (cikm_cup:67) INFO: Loading CIKMCUP data for Client #6.
2022-09-03 16:40:14,227 (cikm_cup:67) INFO: Loading CIKMCUP data for Client #7.
2022-09-03 16:40:14,614 (cikm_cup:67) INFO: Loading CIKMCUP data for Client #8.
2022-09-03 16:40:14,728 (cikm_cup:67) INFO: Loading CIKMCUP data for Client #9.
2022-09-03 16:40:41,910 (cikm_cup:67) INFO: Loading CIKMCUP data for Client #10.
2022-09-03 16:41:05,866 (cikm_cup:67) INFO: Loading CIKMCUP data for Client #11.
2022-09-03 16:41:06,188 (cikm_cup:67) INFO: Loading CIKMCUP data for Client #12.
2022-09-03 16:41:06,268 (cikm_cup:67) INFO: Loading CIKMCUP data for Client #13.
2022-09-03 16:41:23,027 (config:261) INFO: the used configs are: 
asyn:
  min_received_num: 13
  min_received_rate: -1.0
  timeout: 0
  use: True
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  info_diff_type: l2
  inject_round: 0
  max_ite: 400
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  target_label_ind: -1
backend: torch
cfg_file: 
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  consistent_label_distribution: False
  drop_last: False
  graphsaint:
    num_steps: 30
    walk_length: 2
  loader: 
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0.1, 0.1]
  splitter: 
  splitter_args: []
  subsample: 1.0
  target_transform: []
  transform: []
  type: cikmcup
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 20
  the_smaller_the_better: True
eval:
  best_res_update_round_wise_key: val_imp_ratio
  count_flops: False
  freq: 5
  metrics: ['imp_ratio', 'acc']
  monitoring: []
  report: ['avg']
  save_data: False
  split: ['test', 'val']
expname: fedem_gin_on_cikmcup_lr0.25_lstep16_
expname_tag: 
federate:
  client_num: 13
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  method: fedem
  mode: standalone
  online_aggr: False
  restore_from: 
  sample_client_num: 13
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    num_arms: 16
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  log_scale: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  plot_interval: 1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    elim_round_num: 3
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
    ss: 
  working_folder: hpo
model:
  dropout: 0.3
  embed_size: 8
  graph_pooling: mean
  hidden: 64
  in_channels: 0
  layer: 2
  model_num_per_trainer: 1
  num_item: 0
  num_user: 0
  out_channels: 0
  task: graph
  type: gin
  use_bias: True
nbafl:
  use: False
outdir: exp\fedem_gin_on_cikmcup_lr0.25_lstep16_\sub_exp_20220903164012
personalization:
  K: 5
  beta: 1.0
  local_param: ['encoder_atom', 'encoder', 'clf', 'norms', 'linear']
  local_update_steps: 16
  lr: 0.25
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  local_update_steps: 16
  optimizer:
    lr: 0.25
    momentum: 0.9
    type: SGD
    weight_decay: 0.0005
trainer:
  type: flitplustrainer
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2022-09-03 16:41:23,034 (worker_builder:73) WARNING: Server for method fedem is not implemented. Will use default one
2022-09-03 16:41:23,049 (worker_builder:22) WARNING: Clients for method fedem is not implemented. Will use default one
2022-09-03 16:41:23,643 (aggregator_builder:21) WARNING: Aggregator for method fedem is not implemented. Will use default one
2022-09-03 16:41:23,644 (fed_runner:249) INFO: Server #0 has been set up ... 
2022-09-03 16:41:23,685 (config:261) INFO: the used configs are: 
asyn:
  min_received_num: 13
  min_received_rate: -1.0
  timeout: 0
  use: True
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  info_diff_type: l2
  inject_round: 0
  max_ite: 400
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  target_label_ind: -1
backend: torch
cfg_file: 
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  consistent_label_distribution: False
  drop_last: False
  graphsaint:
    num_steps: 30
    walk_length: 2
  loader: 
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0.1, 0.1]
  splitter: 
  splitter_args: []
  subsample: 1.0
  target_transform: []
  transform: []
  type: cikmcup
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 20
  the_smaller_the_better: True
eval:
  base: 0.263789
  best_res_update_round_wise_key: val_imp_ratio
  count_flops: False
  freq: 5
  metrics: ['imp_ratio', 'acc']
  monitoring: []
  report: ['avg']
  save_data: False
  split: ['test', 'val']
expname: fedem_gin_on_cikmcup_lr0.25_lstep16_
expname_tag: 
federate:
  client_num: 13
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  method: fedem
  mode: standalone
  online_aggr: False
  restore_from: 
  sample_client_num: 13
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    num_arms: 16
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  log_scale: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  plot_interval: 1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    elim_round_num: 3
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
    ss: 
  working_folder: hpo
model:
  dropout: 0.3
  embed_size: 8
  graph_pooling: mean
  hidden: 64
  in_channels: 0
  layer: 2
  model_num_per_trainer: 1
  num_item: 0
  num_user: 0
  out_channels: 2
  task: graphClassification
  type: gin
  use_bias: True
nbafl:
  use: False
outdir: exp\fedem_gin_on_cikmcup_lr0.25_lstep16_\sub_exp_20220903164012
personalization:
  K: 5
  beta: 1.0
  local_param: ['encoder_atom', 'encoder', 'clf', 'norms', 'linear']
  local_update_steps: 16
  lr: 0.25
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  local_update_steps: 16
  optimizer:
    lr: 0.05
    momentum: 0.9
    type: SGD
    weight_decay: 0.0005
trainer:
  type: flitplustrainer
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2022-09-03 16:41:23,930 (trainer:142) INFO: Remove the hook `_hook_on_fit_end` from hooks_set at trigger `on_fit_end`
2022-09-03 16:41:24,061 (fed_runner:302) INFO: Client 1 has been set up ... 
2022-09-03 16:41:24,081 (config:261) INFO: the used configs are: 
asyn:
  min_received_num: 13
  min_received_rate: -1.0
  timeout: 0
  use: True
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  info_diff_type: l2
  inject_round: 0
  max_ite: 400
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  target_label_ind: -1
backend: torch
cfg_file: 
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  consistent_label_distribution: False
  drop_last: False
  graphsaint:
    num_steps: 30
    walk_length: 2
  loader: 
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0.1, 0.1]
  splitter: 
  splitter_args: []
  subsample: 1.0
  target_transform: []
  transform: []
  type: cikmcup
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 20
  the_smaller_the_better: True
eval:
  base: 0.289617
  best_res_update_round_wise_key: val_imp_ratio
  count_flops: False
  freq: 5
  metrics: ['imp_ratio', 'acc']
  monitoring: []
  report: ['avg']
  save_data: False
  split: ['test', 'val']
expname: fedem_gin_on_cikmcup_lr0.25_lstep16_
expname_tag: 
federate:
  client_num: 13
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  method: fedem
  mode: standalone
  online_aggr: False
  restore_from: 
  sample_client_num: 13
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    num_arms: 16
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  log_scale: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  plot_interval: 1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    elim_round_num: 3
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
    ss: 
  working_folder: hpo
model:
  dropout: 0.3
  embed_size: 8
  graph_pooling: mean
  hidden: 64
  in_channels: 0
  layer: 2
  model_num_per_trainer: 1
  num_item: 0
  num_user: 0
  out_channels: 2
  task: graphClassification
  type: gin
  use_bias: True
nbafl:
  use: False
outdir: exp\fedem_gin_on_cikmcup_lr0.25_lstep16_\sub_exp_20220903164012
personalization:
  K: 5
  beta: 1.0
  local_param: ['encoder_atom', 'encoder', 'clf', 'norms', 'linear']
  local_update_steps: 16
  lr: 0.25
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  local_update_steps: 16
  optimizer:
    lr: 0.003
    momentum: 0.9
    type: SGD
    weight_decay: 0.0005
trainer:
  type: flitplustrainer
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2022-09-03 16:41:24,138 (trainer:142) INFO: Remove the hook `_hook_on_fit_end` from hooks_set at trigger `on_fit_end`
2022-09-03 16:41:24,139 (fed_runner:302) INFO: Client 2 has been set up ... 
2022-09-03 16:41:24,160 (config:261) INFO: the used configs are: 
asyn:
  min_received_num: 13
  min_received_rate: -1.0
  timeout: 0
  use: True
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  info_diff_type: l2
  inject_round: 0
  max_ite: 400
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  target_label_ind: -1
backend: torch
cfg_file: 
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  consistent_label_distribution: False
  drop_last: False
  graphsaint:
    num_steps: 30
    walk_length: 2
  loader: 
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0.1, 0.1]
  splitter: 
  splitter_args: []
  subsample: 1.0
  target_transform: []
  transform: []
  type: cikmcup
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 20
  the_smaller_the_better: True
eval:
  base: 0.355404
  best_res_update_round_wise_key: val_imp_ratio
  count_flops: False
  freq: 5
  metrics: ['imp_ratio', 'acc']
  monitoring: []
  report: ['avg']
  save_data: False
  split: ['test', 'val']
expname: fedem_gin_on_cikmcup_lr0.25_lstep16_
expname_tag: 
federate:
  client_num: 13
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  method: fedem
  mode: standalone
  online_aggr: False
  restore_from: 
  sample_client_num: 13
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    num_arms: 16
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  log_scale: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  plot_interval: 1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    elim_round_num: 3
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
    ss: 
  working_folder: hpo
model:
  dropout: 0.3
  embed_size: 8
  graph_pooling: mean
  hidden: 64
  in_channels: 0
  layer: 2
  model_num_per_trainer: 1
  num_item: 0
  num_user: 0
  out_channels: 2
  task: graphClassification
  type: gin
  use_bias: True
nbafl:
  use: False
outdir: exp\fedem_gin_on_cikmcup_lr0.25_lstep16_\sub_exp_20220903164012
personalization:
  K: 5
  beta: 1.0
  local_param: ['encoder_atom', 'encoder', 'clf', 'norms', 'linear']
  local_update_steps: 16
  lr: 0.25
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  local_update_steps: 16
  optimizer:
    lr: 0.0004
    momentum: 0.9
    type: SGD
    weight_decay: 0.0005
trainer:
  type: flitplustrainer
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2022-09-03 16:41:24,433 (trainer:142) INFO: Remove the hook `_hook_on_fit_end` from hooks_set at trigger `on_fit_end`
2022-09-03 16:41:24,434 (fed_runner:302) INFO: Client 3 has been set up ... 
2022-09-03 16:41:24,453 (config:261) INFO: the used configs are: 
asyn:
  min_received_num: 13
  min_received_rate: -1.0
  timeout: 0
  use: True
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  info_diff_type: l2
  inject_round: 0
  max_ite: 400
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  target_label_ind: -1
backend: torch
cfg_file: 
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  consistent_label_distribution: False
  drop_last: False
  graphsaint:
    num_steps: 30
    walk_length: 2
  loader: 
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0.1, 0.1]
  splitter: 
  splitter_args: []
  subsample: 1.0
  target_transform: []
  transform: []
  type: cikmcup
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 20
  the_smaller_the_better: True
eval:
  base: 0.176471
  best_res_update_round_wise_key: val_imp_ratio
  count_flops: False
  freq: 5
  metrics: ['imp_ratio', 'acc']
  monitoring: []
  report: ['avg']
  save_data: False
  split: ['test', 'val']
expname: fedem_gin_on_cikmcup_lr0.25_lstep16_
expname_tag: 
federate:
  client_num: 13
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  method: fedem
  mode: standalone
  online_aggr: False
  restore_from: 
  sample_client_num: 13
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    num_arms: 16
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  log_scale: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  plot_interval: 1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    elim_round_num: 3
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
    ss: 
  working_folder: hpo
model:
  dropout: 0.3
  embed_size: 8
  graph_pooling: mean
  hidden: 64
  in_channels: 0
  layer: 2
  model_num_per_trainer: 1
  num_item: 0
  num_user: 0
  out_channels: 2
  task: graphClassification
  type: gin
  use_bias: True
nbafl:
  use: False
outdir: exp\fedem_gin_on_cikmcup_lr0.25_lstep16_\sub_exp_20220903164012
personalization:
  K: 5
  beta: 1.0
  local_param: ['encoder_atom', 'encoder', 'clf', 'norms', 'linear']
  local_update_steps: 16
  lr: 0.25
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  local_update_steps: 16
  optimizer:
    lr: 0.005
    momentum: 0.9
    type: SGD
    weight_decay: 0.0005
trainer:
  type: flitplustrainer
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2022-09-03 16:41:24,490 (trainer:142) INFO: Remove the hook `_hook_on_fit_end` from hooks_set at trigger `on_fit_end`
2022-09-03 16:41:24,491 (fed_runner:302) INFO: Client 4 has been set up ... 
2022-09-03 16:41:24,513 (config:261) INFO: the used configs are: 
asyn:
  min_received_num: 13
  min_received_rate: -1.0
  timeout: 0
  use: True
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  info_diff_type: l2
  inject_round: 0
  max_ite: 400
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  target_label_ind: -1
backend: torch
cfg_file: 
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  consistent_label_distribution: False
  drop_last: False
  graphsaint:
    num_steps: 30
    walk_length: 2
  loader: 
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0.1, 0.1]
  splitter: 
  splitter_args: []
  subsample: 1.0
  target_transform: []
  transform: []
  type: cikmcup
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 20
  the_smaller_the_better: True
eval:
  base: 0.396825
  best_res_update_round_wise_key: val_imp_ratio
  count_flops: False
  freq: 5
  metrics: ['imp_ratio', 'acc']
  monitoring: []
  report: ['avg']
  save_data: False
  split: ['test', 'val']
expname: fedem_gin_on_cikmcup_lr0.25_lstep16_
expname_tag: 
federate:
  client_num: 13
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  method: fedem
  mode: standalone
  online_aggr: False
  restore_from: 
  sample_client_num: 13
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    num_arms: 16
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  log_scale: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  plot_interval: 1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    elim_round_num: 3
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
    ss: 
  working_folder: hpo
model:
  dropout: 0.3
  embed_size: 8
  graph_pooling: mean
  hidden: 64
  in_channels: 0
  layer: 2
  model_num_per_trainer: 1
  num_item: 0
  num_user: 0
  out_channels: 2
  task: graphClassification
  type: gin
  use_bias: True
nbafl:
  use: False
outdir: exp\fedem_gin_on_cikmcup_lr0.25_lstep16_\sub_exp_20220903164012
personalization:
  K: 5
  beta: 1.0
  local_param: ['encoder_atom', 'encoder', 'clf', 'norms', 'linear']
  local_update_steps: 16
  lr: 0.25
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  local_update_steps: 16
  optimizer:
    lr: 8e-05
    momentum: 0.9
    type: SGD
    weight_decay: 0.0005
trainer:
  type: flitplustrainer
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2022-09-03 16:41:24,568 (trainer:142) INFO: Remove the hook `_hook_on_fit_end` from hooks_set at trigger `on_fit_end`
2022-09-03 16:41:24,569 (fed_runner:302) INFO: Client 5 has been set up ... 
2022-09-03 16:41:24,589 (config:261) INFO: the used configs are: 
asyn:
  min_received_num: 13
  min_received_rate: -1.0
  timeout: 0
  use: True
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  info_diff_type: l2
  inject_round: 0
  max_ite: 400
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  target_label_ind: -1
backend: torch
cfg_file: 
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  consistent_label_distribution: False
  drop_last: False
  graphsaint:
    num_steps: 30
    walk_length: 2
  loader: 
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0.1, 0.1]
  splitter: 
  splitter_args: []
  subsample: 1.0
  target_transform: []
  transform: []
  type: cikmcup
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 20
  the_smaller_the_better: True
eval:
  base: 0.26158
  best_res_update_round_wise_key: val_imp_ratio
  count_flops: False
  freq: 5
  metrics: ['imp_ratio', 'acc']
  monitoring: []
  report: ['avg']
  save_data: False
  split: ['test', 'val']
expname: fedem_gin_on_cikmcup_lr0.25_lstep16_
expname_tag: 
federate:
  client_num: 13
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  method: fedem
  mode: standalone
  online_aggr: False
  restore_from: 
  sample_client_num: 13
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    num_arms: 16
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  log_scale: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  plot_interval: 1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    elim_round_num: 3
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
    ss: 
  working_folder: hpo
model:
  dropout: 0.3
  embed_size: 8
  graph_pooling: mean
  hidden: 64
  in_channels: 0
  layer: 2
  model_num_per_trainer: 1
  num_item: 0
  num_user: 0
  out_channels: 2
  task: graphClassification
  type: gin
  use_bias: True
nbafl:
  use: False
outdir: exp\fedem_gin_on_cikmcup_lr0.25_lstep16_\sub_exp_20220903164012
personalization:
  K: 5
  beta: 1.0
  local_param: ['encoder_atom', 'encoder', 'clf', 'norms', 'linear']
  local_update_steps: 16
  lr: 0.25
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  local_update_steps: 16
  optimizer:
    lr: 0.0005
    momentum: 0.9
    type: SGD
    weight_decay: 0.0005
trainer:
  type: flitplustrainer
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2022-09-03 16:41:24,791 (trainer:142) INFO: Remove the hook `_hook_on_fit_end` from hooks_set at trigger `on_fit_end`
2022-09-03 16:41:24,791 (fed_runner:302) INFO: Client 6 has been set up ... 
2022-09-03 16:41:24,811 (config:261) INFO: the used configs are: 
asyn:
  min_received_num: 13
  min_received_rate: -1.0
  timeout: 0
  use: True
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  info_diff_type: l2
  inject_round: 0
  max_ite: 400
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  target_label_ind: -1
backend: torch
cfg_file: 
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  consistent_label_distribution: False
  drop_last: False
  graphsaint:
    num_steps: 30
    walk_length: 2
  loader: 
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0.1, 0.1]
  splitter: 
  splitter_args: []
  subsample: 1.0
  target_transform: []
  transform: []
  type: cikmcup
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 20
  the_smaller_the_better: True
eval:
  base: 0.302378
  best_res_update_round_wise_key: val_imp_ratio
  count_flops: False
  freq: 5
  metrics: ['imp_ratio', 'acc']
  monitoring: []
  report: ['avg']
  save_data: False
  split: ['test', 'val']
expname: fedem_gin_on_cikmcup_lr0.25_lstep16_
expname_tag: 
federate:
  client_num: 13
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  method: fedem
  mode: standalone
  online_aggr: False
  restore_from: 
  sample_client_num: 13
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    num_arms: 16
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  log_scale: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  plot_interval: 1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    elim_round_num: 3
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
    ss: 
  working_folder: hpo
model:
  dropout: 0.3
  embed_size: 8
  graph_pooling: mean
  hidden: 64
  in_channels: 0
  layer: 2
  model_num_per_trainer: 1
  num_item: 0
  num_user: 0
  out_channels: 2
  task: graphClassification
  type: gin
  use_bias: True
nbafl:
  use: False
outdir: exp\fedem_gin_on_cikmcup_lr0.25_lstep16_\sub_exp_20220903164012
personalization:
  K: 5
  beta: 1.0
  local_param: ['encoder_atom', 'encoder', 'clf', 'norms', 'linear']
  local_update_steps: 16
  lr: 0.25
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  local_update_steps: 16
  optimizer:
    lr: 0.005
    momentum: 0.9
    type: SGD
    weight_decay: 0.0005
trainer:
  type: flitplustrainer
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2022-09-03 16:41:25,116 (trainer:142) INFO: Remove the hook `_hook_on_fit_end` from hooks_set at trigger `on_fit_end`
2022-09-03 16:41:25,117 (fed_runner:302) INFO: Client 7 has been set up ... 
2022-09-03 16:41:25,161 (config:261) INFO: the used configs are: 
asyn:
  min_received_num: 13
  min_received_rate: -1.0
  timeout: 0
  use: True
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  info_diff_type: l2
  inject_round: 0
  max_ite: 400
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  target_label_ind: -1
backend: torch
cfg_file: 
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  consistent_label_distribution: False
  drop_last: False
  graphsaint:
    num_steps: 30
    walk_length: 2
  loader: 
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0.1, 0.1]
  splitter: 
  splitter_args: []
  subsample: 1.0
  target_transform: []
  transform: []
  type: cikmcup
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 20
  the_smaller_the_better: True
eval:
  base: 0.211538
  best_res_update_round_wise_key: val_imp_ratio
  count_flops: False
  freq: 5
  metrics: ['imp_ratio', 'acc']
  monitoring: []
  report: ['avg']
  save_data: False
  split: ['test', 'val']
expname: fedem_gin_on_cikmcup_lr0.25_lstep16_
expname_tag: 
federate:
  client_num: 13
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  method: fedem
  mode: standalone
  online_aggr: False
  restore_from: 
  sample_client_num: 13
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    num_arms: 16
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  log_scale: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  plot_interval: 1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    elim_round_num: 3
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
    ss: 
  working_folder: hpo
model:
  dropout: 0.3
  embed_size: 8
  graph_pooling: mean
  hidden: 64
  in_channels: 0
  layer: 2
  model_num_per_trainer: 1
  num_item: 0
  num_user: 0
  out_channels: 2
  task: graphClassification
  type: gin
  use_bias: True
nbafl:
  use: False
outdir: exp\fedem_gin_on_cikmcup_lr0.25_lstep16_\sub_exp_20220903164012
personalization:
  K: 5
  beta: 1.0
  local_param: ['encoder_atom', 'encoder', 'clf', 'norms', 'linear']
  local_update_steps: 16
  lr: 0.25
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  local_update_steps: 16
  optimizer:
    lr: 0.01
    momentum: 0.9
    type: SGD
    weight_decay: 0.0005
trainer:
  type: flitplustrainer
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2022-09-03 16:41:25,308 (trainer:142) INFO: Remove the hook `_hook_on_fit_end` from hooks_set at trigger `on_fit_end`
2022-09-03 16:41:25,309 (fed_runner:302) INFO: Client 8 has been set up ... 
2022-09-03 16:41:25,353 (config:261) INFO: the used configs are: 
asyn:
  min_received_num: 13
  min_received_rate: -1.0
  timeout: 0
  use: True
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  info_diff_type: l2
  inject_round: 0
  max_ite: 400
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  target_label_ind: -1
backend: torch
cfg_file: 
criterion:
  type: MSELoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  consistent_label_distribution: False
  drop_last: False
  graphsaint:
    num_steps: 30
    walk_length: 2
  loader: 
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0.1, 0.1]
  splitter: 
  splitter_args: []
  subsample: 1.0
  target_transform: []
  transform: []
  type: cikmcup
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 20
  the_smaller_the_better: True
eval:
  base: 0.059199
  best_res_update_round_wise_key: val_imp_ratio
  count_flops: False
  freq: 5
  metrics: ['imp_ratio', 'mse']
  monitoring: []
  report: ['avg']
  save_data: False
  split: ['test', 'val']
expname: fedem_gin_on_cikmcup_lr0.25_lstep16_
expname_tag: 
federate:
  client_num: 13
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  method: fedem
  mode: standalone
  online_aggr: False
  restore_from: 
  sample_client_num: 13
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    num_arms: 16
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  log_scale: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  plot_interval: 1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    elim_round_num: 3
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
    ss: 
  working_folder: hpo
model:
  dropout: 0.3
  embed_size: 8
  graph_pooling: mean
  hidden: 64
  in_channels: 0
  layer: 2
  model_num_per_trainer: 1
  num_item: 0
  num_user: 0
  out_channels: 1
  task: graphRegression
  type: gin
  use_bias: True
nbafl:
  use: False
outdir: exp\fedem_gin_on_cikmcup_lr0.25_lstep16_\sub_exp_20220903164012
personalization:
  K: 5
  beta: 1.0
  local_param: ['encoder_atom', 'encoder', 'clf', 'norms', 'linear']
  local_update_steps: 16
  lr: 0.25
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  local_update_steps: 16
  optimizer:
    lr: 0.05
    momentum: 0.9
    type: SGD
    weight_decay: 0.0005
trainer:
  type: flitplustrainer
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2022-09-03 16:41:54,566 (trainer:142) INFO: Remove the hook `_hook_on_fit_end` from hooks_set at trigger `on_fit_end`
2022-09-03 16:41:54,568 (fed_runner:302) INFO: Client 9 has been set up ... 
2022-09-03 16:41:54,612 (config:261) INFO: the used configs are: 
asyn:
  min_received_num: 13
  min_received_rate: -1.0
  timeout: 0
  use: True
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  info_diff_type: l2
  inject_round: 0
  max_ite: 400
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  target_label_ind: -1
backend: torch
cfg_file: 
criterion:
  type: MSELoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  consistent_label_distribution: False
  drop_last: False
  graphsaint:
    num_steps: 30
    walk_length: 2
  loader: 
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0.1, 0.1]
  splitter: 
  splitter_args: []
  subsample: 1.0
  target_transform: []
  transform: []
  type: cikmcup
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 20
  the_smaller_the_better: True
eval:
  base: 0.007083
  best_res_update_round_wise_key: val_imp_ratio
  count_flops: False
  freq: 5
  metrics: ['imp_ratio', 'mse']
  monitoring: []
  report: ['avg']
  save_data: False
  split: ['test', 'val']
expname: fedem_gin_on_cikmcup_lr0.25_lstep16_
expname_tag: 
federate:
  client_num: 13
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  method: fedem
  mode: standalone
  online_aggr: False
  restore_from: 
  sample_client_num: 13
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    num_arms: 16
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  log_scale: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  plot_interval: 1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    elim_round_num: 3
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
    ss: 
  working_folder: hpo
model:
  dropout: 0.3
  embed_size: 8
  graph_pooling: mean
  hidden: 64
  in_channels: 0
  layer: 2
  model_num_per_trainer: 1
  num_item: 0
  num_user: 0
  out_channels: 10
  task: graphRegression
  type: gin
  use_bias: True
nbafl:
  use: False
outdir: exp\fedem_gin_on_cikmcup_lr0.25_lstep16_\sub_exp_20220903164012
personalization:
  K: 5
  beta: 1.0
  local_param: ['encoder_atom', 'encoder', 'clf', 'norms', 'linear']
  local_update_steps: 16
  lr: 0.25
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  local_update_steps: 16
  optimizer:
    lr: 0.02
    momentum: 0.9
    type: SGD
    weight_decay: 0.0005
trainer:
  type: flitplustrainer
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2022-09-03 16:42:16,481 (trainer:142) INFO: Remove the hook `_hook_on_fit_end` from hooks_set at trigger `on_fit_end`
2022-09-03 16:42:16,482 (fed_runner:302) INFO: Client 10 has been set up ... 
2022-09-03 16:42:16,504 (config:261) INFO: the used configs are: 
asyn:
  min_received_num: 13
  min_received_rate: -1.0
  timeout: 0
  use: True
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  info_diff_type: l2
  inject_round: 0
  max_ite: 400
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  target_label_ind: -1
backend: torch
cfg_file: 
criterion:
  type: MSELoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  consistent_label_distribution: False
  drop_last: False
  graphsaint:
    num_steps: 30
    walk_length: 2
  loader: 
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0.1, 0.1]
  splitter: 
  splitter_args: []
  subsample: 1.0
  target_transform: []
  transform: []
  type: cikmcup
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 20
  the_smaller_the_better: True
eval:
  base: 0.734011
  best_res_update_round_wise_key: val_imp_ratio
  count_flops: False
  freq: 5
  metrics: ['imp_ratio', 'mse']
  monitoring: []
  report: ['avg']
  save_data: False
  split: ['test', 'val']
expname: fedem_gin_on_cikmcup_lr0.25_lstep16_
expname_tag: 
federate:
  client_num: 13
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  method: fedem
  mode: standalone
  online_aggr: False
  restore_from: 
  sample_client_num: 13
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    num_arms: 16
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  log_scale: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  plot_interval: 1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    elim_round_num: 3
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
    ss: 
  working_folder: hpo
model:
  dropout: 0.3
  embed_size: 8
  graph_pooling: mean
  hidden: 64
  in_channels: 0
  layer: 2
  model_num_per_trainer: 1
  num_item: 0
  num_user: 0
  out_channels: 1
  task: graphRegression
  type: gin
  use_bias: True
nbafl:
  use: False
outdir: exp\fedem_gin_on_cikmcup_lr0.25_lstep16_\sub_exp_20220903164012
personalization:
  K: 5
  beta: 1.0
  local_param: ['encoder_atom', 'encoder', 'clf', 'norms', 'linear']
  local_update_steps: 16
  lr: 0.25
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  local_update_steps: 16
  optimizer:
    lr: 0.02
    momentum: 0.9
    type: SGD
    weight_decay: 0.0005
trainer:
  type: flitplustrainer
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2022-09-03 16:42:16,857 (trainer:142) INFO: Remove the hook `_hook_on_fit_end` from hooks_set at trigger `on_fit_end`
2022-09-03 16:42:16,859 (fed_runner:302) INFO: Client 11 has been set up ... 
2022-09-03 16:42:16,881 (config:261) INFO: the used configs are: 
asyn:
  min_received_num: 13
  min_received_rate: -1.0
  timeout: 0
  use: True
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  info_diff_type: l2
  inject_round: 0
  max_ite: 400
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  target_label_ind: -1
backend: torch
cfg_file: 
criterion:
  type: MSELoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  consistent_label_distribution: False
  drop_last: False
  graphsaint:
    num_steps: 30
    walk_length: 2
  loader: 
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0.1, 0.1]
  splitter: 
  splitter_args: []
  subsample: 1.0
  target_transform: []
  transform: []
  type: cikmcup
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 20
  the_smaller_the_better: True
eval:
  base: 1.361326
  best_res_update_round_wise_key: val_imp_ratio
  count_flops: False
  freq: 5
  metrics: ['imp_ratio', 'mse']
  monitoring: []
  report: ['avg']
  save_data: False
  split: ['test', 'val']
expname: fedem_gin_on_cikmcup_lr0.25_lstep16_
expname_tag: 
federate:
  client_num: 13
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  method: fedem
  mode: standalone
  online_aggr: False
  restore_from: 
  sample_client_num: 13
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    num_arms: 16
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  log_scale: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  plot_interval: 1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    elim_round_num: 3
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
    ss: 
  working_folder: hpo
model:
  dropout: 0.3
  embed_size: 8
  graph_pooling: mean
  hidden: 64
  in_channels: 0
  layer: 2
  model_num_per_trainer: 1
  num_item: 0
  num_user: 0
  out_channels: 1
  task: graphRegression
  type: gin
  use_bias: True
nbafl:
  use: False
outdir: exp\fedem_gin_on_cikmcup_lr0.25_lstep16_\sub_exp_20220903164012
personalization:
  K: 5
  beta: 1.0
  local_param: ['encoder_atom', 'encoder', 'clf', 'norms', 'linear']
  local_update_steps: 16
  lr: 0.25
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  local_update_steps: 16
  optimizer:
    lr: 0.004
    momentum: 0.9
    type: SGD
    weight_decay: 0.0005
trainer:
  type: flitplustrainer
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2022-09-03 16:42:17,006 (trainer:142) INFO: Remove the hook `_hook_on_fit_end` from hooks_set at trigger `on_fit_end`
2022-09-03 16:42:17,008 (fed_runner:302) INFO: Client 12 has been set up ... 
2022-09-03 16:42:17,026 (config:261) INFO: the used configs are: 
asyn:
  min_received_num: 13
  min_received_rate: -1.0
  timeout: 0
  use: True
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  info_diff_type: l2
  inject_round: 0
  max_ite: 400
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  target_label_ind: -1
backend: torch
cfg_file: 
criterion:
  type: MSELoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  consistent_label_distribution: False
  drop_last: False
  graphsaint:
    num_steps: 30
    walk_length: 2
  loader: 
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.8, 0.1, 0.1]
  splitter: 
  splitter_args: []
  subsample: 1.0
  target_transform: []
  transform: []
  type: cikmcup
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 20
  the_smaller_the_better: True
eval:
  base: 0.004389
  best_res_update_round_wise_key: val_imp_ratio
  count_flops: False
  freq: 5
  metrics: ['imp_ratio', 'mse']
  monitoring: []
  report: ['avg']
  save_data: False
  split: ['test', 'val']
expname: fedem_gin_on_cikmcup_lr0.25_lstep16_
expname_tag: 
federate:
  client_num: 13
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  method: fedem
  mode: standalone
  online_aggr: False
  restore_from: 
  sample_client_num: 13
  sample_client_rate: -1.0
  sampler: uniform
  save_to: 
  share_local_model: False
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
finetune:
  batch_or_epoch: epoch
  before_eval: False
  freeze_param: 
  local_update_steps: 1
  optimizer:
    lr: 0.1
    type: SGD
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    num_arms: 16
    sched: auto
    ss: 
    use: False
  init_cand_num: 16
  larger_better: False
  log_scale: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  plot_interval: 1
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    elim_round_num: 3
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
    ss: 
  working_folder: hpo
model:
  dropout: 0.3
  embed_size: 8
  graph_pooling: mean
  hidden: 64
  in_channels: 0
  layer: 2
  model_num_per_trainer: 1
  num_item: 0
  num_user: 0
  out_channels: 12
  task: graphRegression
  type: gin
  use_bias: True
nbafl:
  use: False
outdir: exp\fedem_gin_on_cikmcup_lr0.25_lstep16_\sub_exp_20220903164012
personalization:
  K: 5
  beta: 1.0
  local_param: ['encoder_atom', 'encoder', 'clf', 'norms', 'linear']
  local_update_steps: 16
  lr: 0.25
  regular_weight: 0.1
  share_non_trainable_para: False
print_decimal_digits: 6
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  local_update_steps: 16
  optimizer:
    lr: 0.02
    momentum: 0.9
    type: SGD
    weight_decay: 0.0005
trainer:
  type: flitplustrainer
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2022-09-03 16:42:27,843 (trainer:142) INFO: Remove the hook `_hook_on_fit_end` from hooks_set at trigger `on_fit_end`
2022-09-03 16:42:27,845 (fed_runner:302) INFO: Client 13 has been set up ... 
2022-09-03 16:42:27,846 (trainer:324) INFO: Model meta-info: <class 'federatedscope.gfl.model.graph_level.GNN_Net_Graph'>.
2022-09-03 16:42:27,847 (trainer:332) INFO: Num of original para names: 58.
2022-09-03 16:42:27,847 (trainer:333) INFO: Num of original trainable para names: 44.
2022-09-03 16:42:27,848 (trainer:335) INFO: Num of preserved para names in local update: 2. 
Preserved para names in local update: {'gnn.convs.1.eps', 'gnn.convs.0.eps'}.
2022-09-03 16:42:27,848 (trainer:339) INFO: Num of filtered para names in local update: 56. 
Filtered para names in local update: {'gnn.convs.0.nn.norms.0.running_mean', 'encoder_atom.atom_embedding_list.6.weight', 'linear.0.weight', 'encoder_atom.atom_embedding_list.1.weight', 'gnn.convs.0.nn.norms.0.weight', 'encoder_atom.atom_embedding_list.2.weight', 'gnn.convs.1.nn.norms.0.bias', 'encoder_atom.atom_embedding_list.13.weight', 'encoder_atom.atom_embedding_list.0.weight', 'gnn.convs.0.nn.linears.0.bias', 'gnn.convs.1.nn.linears.0.weight', 'encoder_atom.atom_embedding_list.16.weight', 'encoder_atom.atom_embedding_list.5.weight', 'encoder_atom.atom_embedding_list.18.weight', 'encoder_atom.atom_embedding_list.9.weight', 'gnn.convs.0.nn.norms.1.num_batches_tracked', 'gnn.convs.1.nn.norms.1.bias', 'gnn.convs.1.nn.norms.1.num_batches_tracked', 'encoder_atom.atom_embedding_list.14.weight', 'encoder.bias', 'gnn.convs.1.nn.norms.0.running_mean', 'encoder.weight', 'gnn.convs.1.nn.norms.0.num_batches_tracked', 'encoder_atom.atom_embedding_list.17.weight', 'gnn.convs.0.nn.linears.1.weight', 'encoder_atom.atom_embedding_list.7.weight', 'gnn.convs.0.nn.linears.0.weight', 'encoder_atom.atom_embedding_list.3.weight', 'gnn.convs.0.nn.norms.1.bias', 'gnn.convs.0.nn.norms.0.num_batches_tracked', 'encoder_atom.atom_embedding_list.12.weight', 'gnn.convs.0.nn.norms.1.running_mean', 'encoder_atom.atom_embedding_list.4.weight', 'encoder_atom.atom_embedding_list.15.weight', 'gnn.convs.1.nn.norms.0.running_var', 'clf.bias', 'gnn.convs.1.nn.norms.0.weight', 'encoder_atom.atom_embedding_list.21.weight', 'gnn.convs.0.nn.norms.0.bias', 'gnn.convs.1.nn.linears.0.bias', 'encoder_atom.atom_embedding_list.10.weight', 'clf.weight', 'encoder_atom.atom_embedding_list.8.weight', 'gnn.convs.1.nn.linears.1.weight', 'gnn.convs.0.nn.norms.0.running_var', 'gnn.convs.1.nn.norms.1.weight', 'gnn.convs.1.nn.norms.1.running_mean', 'encoder_atom.atom_embedding_list.19.weight', 'encoder_atom.atom_embedding_list.20.weight', 'gnn.convs.1.nn.norms.1.running_var', 'gnn.convs.0.nn.norms.1.weight', 'gnn.convs.0.nn.norms.1.running_var', 'linear.0.bias', 'gnn.convs.0.nn.linears.1.bias', 'gnn.convs.1.nn.linears.1.bias', 'encoder_atom.atom_embedding_list.11.weight'}.
2022-09-03 16:42:27,849 (trainer:344) INFO: After register default hooks,
	the hooks_in_train is:
	[
	  {
	    "on_fit_start": [
	      "hook_on_fit_start_mixture_weights_update",
	      "_hook_on_fit_start_flop_count",
	      "_hook_on_fit_start_init",
	      "_hook_on_fit_start_calculate_model_size",
	      "record_initialization_local",
	      "record_initialization_global"
	    ],
	    "on_epoch_start": [
	      "_hook_on_epoch_start"
	    ],
	    "on_batch_start": [
	      "hook_on_batch_start_track_batch_idx",
	      "_hook_on_batch_start_init"
	    ],
	    "on_batch_forward": [
	      "_hook_on_batch_forward",
	      "_hook_on_batch_forward_regularizer",
	      "_hook_on_batch_forward_flop_count",
	      "hook_on_batch_forward_weighted_loss"
	    ],
	    "on_batch_backward": [
	      "_hook_on_batch_backward"
	    ],
	    "on_batch_end": [
	      "_hook_on_batch_end"
	    ],
	    "on_fit_end": [
	      "_hook_on_fit_end",
	      "del_initialization_local",
	      "del_initialization_global",
	      "_hook_on_fit_end_flop_count"
	    ]
	  }
	];
	the hooks_in_eval is:
            t[
	  {
	    "on_fit_start": [
	      "_hook_on_fit_start_init",
	      "record_initialization_local",
	      "record_initialization_global"
	    ],
	    "on_epoch_start": [
	      "_hook_on_epoch_start"
	    ],
	    "on_batch_start": [
	      "hook_on_batch_start_track_batch_idx",
	      "_hook_on_batch_start_init"
	    ],
	    "on_batch_forward": [
	      "_hook_on_batch_forward"
	    ],
	    "on_batch_end": [
	      "hook_on_batch_end_gather_loss",
	      "_hook_on_batch_end"
	    ],
	    "on_fit_end": [
	      "_hook_on_fit_end_ensemble_eval",
	      "del_initialization_local",
	      "del_initialization_global"
	    ]
	  }
	]
2022-09-03 16:42:27,860 (server:635) INFO: ----------- Starting training (Round #0) -------------
2022-09-03 16:42:33,366 (client:259) INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {'train_avg_loss': 1.051354, 'train_loss': 1076.586044, 'train_imp_ratio': -51.79159, 'train_acc': 0.541016, 'train_total': 1024}}
2022-09-03 16:42:35,122 (client:259) INFO: {'Role': 'Client #12', 'Round': 0, 'Results_raw': {'train_avg_loss': 6.795675, 'train_loss': 6741.309677, 'train_mse': 7.169918, 'train_imp_ratio': -426.686338, 'train_total': 992}}
2022-09-03 16:42:36,723 (client:259) INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {'train_avg_loss': 1.12207, 'train_loss': 1126.55823, 'train_imp_ratio': -14.705294, 'train_acc': 0.544821, 'train_total': 1004}}
2022-09-03 16:42:39,060 (client:259) INFO: {'Role': 'Client #11', 'Round': 0, 'Results_raw': {'train_avg_loss': 1.909061, 'train_loss': 1954.878433, 'train_mse': 2.33684, 'train_imp_ratio': -218.365854, 'train_total': 1024}}
2022-09-03 16:42:41,348 (client:259) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {'train_avg_loss': 0.982203, 'train_loss': 1005.775482, 'train_imp_ratio': -22.549795, 'train_acc': 0.564453, 'train_total': 1024}}
2022-09-03 16:43:45,250 (client:259) INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {'train_avg_loss': 0.392469, 'train_loss': 401.888072, 'train_mse': 0.465589, 'train_imp_ratio': -686.48137, 'train_total': 1024}}
2022-09-03 16:43:46,867 (client:259) INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {'train_avg_loss': 1.107798, 'train_loss': 1073.455822, 'train_imp_ratio': -83.866189, 'train_acc': 0.467492, 'train_total': 969}}
2022-09-03 16:43:48,718 (client:259) INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {'train_avg_loss': 0.801571, 'train_loss': 776.722379, 'train_imp_ratio': -11.230189, 'train_acc': 0.764706, 'train_total': 969}}
2022-09-03 16:44:38,261 (client:259) INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {'train_avg_loss': 0.087735, 'train_loss': 89.841056, 'train_mse': 0.104262, 'train_imp_ratio': -1372.00131, 'train_total': 1024}}
2022-09-03 16:44:39,685 (client:259) INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {'train_avg_loss': 1.047893, 'train_loss': 846.697294, 'train_imp_ratio': -127.227193, 'train_acc': 0.59901, 'train_total': 808}}
2022-09-03 16:44:41,438 (client:259) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {'train_avg_loss': 0.925087, 'train_loss': 947.289536, 'train_imp_ratio': -52.154634, 'train_acc': 0.598633, 'train_total': 1024}}
2022-09-03 16:44:43,050 (client:259) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {'train_avg_loss': 1.074896, 'train_loss': 1100.693573, 'train_imp_ratio': -179.252523, 'train_acc': 0.269531, 'train_total': 1024}}
2022-09-03 16:45:14,564 (client:259) INFO: {'Role': 'Client #13', 'Round': 0, 'Results_raw': {'train_avg_loss': 0.108126, 'train_loss': 110.721528, 'train_mse': 0.134702, 'train_imp_ratio': -2969.084784, 'train_total': 1024}}
2022-09-03 16:45:14,566 (server:332) INFO: Server #0: Training is finished! Starting evaluation.
2022-09-03 16:47:00,515 (server:487) INFO: {'Role': 'Server #', 'Round': 1, 'Results_avg': {'test_avg_loss': 1.603017, 'test_loss': 7670.197106, 'test_imp_ratio': -681.415875, 'test_acc': 0.747951, 'test_total': 8350.846154, 'val_avg_loss': 0.650807, 'val_loss': 766.560797, 'val_imp_ratio': -172.482355, 'val_acc': 0.58048, 'val_total': 8350.461538, 'test_mse': 4.052566, 'val_mse': 1.084796}}
2022-09-03 16:47:00,518 (server:386) INFO: Server #0: Final evaluation is finished! Starting merging results.
2022-09-03 16:47:00,519 (server:416) INFO: {'Role': 'Server #', 'Round': 'Final', 'Results_raw': {'client_best_individual': {}, 'client_summarized_avg': {}}}
2022-09-03 16:47:00,521 (server:437) INFO: {'Role': 'Client #1', 'Round': 1, 'Results_raw': {'test_avg_loss': 0.436618, 'test_loss': 182.069566, 'test_imp_ratio': 100.0, 'test_acc': 1.0, 'test_total': 417, 'val_avg_loss': 0.525731, 'val_loss': 218.703957, 'val_imp_ratio': -45.804179, 'val_acc': 0.615385, 'val_total': 416}}
2022-09-03 16:47:00,523 (server:437) INFO: {'Role': 'Client #2', 'Round': 1, 'Results_raw': {'test_avg_loss': 0.540913, 'test_loss': 32.995674, 'test_imp_ratio': 94.339613, 'test_acc': 0.983607, 'test_total': 61, 'val_avg_loss': 0.549762, 'val_loss': 32.985731, 'val_imp_ratio': -43.868166, 'val_acc': 0.583333, 'val_total': 60}}
2022-09-03 16:47:00,525 (server:437) INFO: {'Role': 'Client #3', 'Round': 1, 'Results_raw': {'test_avg_loss': 0.460328, 'test_loss': 340.64262, 'test_imp_ratio': 100.0, 'test_acc': 1.0, 'test_total': 740, 'val_avg_loss': 0.543308, 'val_loss': 402.047797, 'val_imp_ratio': -23.574633, 'val_acc': 0.560811, 'val_total': 740}}
2022-09-03 16:47:00,527 (server:437) INFO: {'Role': 'Client #4', 'Round': 1, 'Results_raw': {'test_avg_loss': 0.668384, 'test_loss': 22.725059, 'test_imp_ratio': -466.665344, 'test_acc': 0.0, 'test_total': 34, 'val_avg_loss': 0.51522, 'val_loss': 17.517478, 'val_imp_ratio': -83.332906, 'val_acc': 0.676471, 'val_total': 34}}
2022-09-03 16:47:00,529 (server:437) INFO: {'Role': 'Client #5', 'Round': 1, 'Results_raw': {'test_avg_loss': 0.502635, 'test_loss': 31.666009, 'test_imp_ratio': 100.0, 'test_acc': 1.0, 'test_total': 63, 'val_avg_loss': 0.546979, 'val_loss': 34.459681, 'val_imp_ratio': -12.000112, 'val_acc': 0.555556, 'val_total': 63}}
2022-09-03 16:47:00,532 (server:437) INFO: {'Role': 'Client #6', 'Round': 1, 'Results_raw': {'test_avg_loss': 0.498236, 'test_loss': 182.852562, 'test_imp_ratio': 100.0, 'test_acc': 1.0, 'test_total': 367, 'val_avg_loss': 0.581979, 'val_loss': 213.586142, 'val_imp_ratio': -189.583756, 'val_acc': 0.242507, 'val_total': 367}}
2022-09-03 16:47:00,533 (server:437) INFO: {'Role': 'Client #7', 'Round': 1, 'Results_raw': {'test_avg_loss': 0.509732, 'test_loss': 378.731172, 'test_imp_ratio': 100.0, 'test_acc': 1.0, 'test_total': 743, 'val_avg_loss': 0.548705, 'val_loss': 407.687997, 'val_imp_ratio': -51.780289, 'val_acc': 0.54105, 'val_total': 743}}
2022-09-03 16:47:00,535 (server:437) INFO: {'Role': 'Client #8', 'Round': 1, 'Results_raw': {'test_avg_loss': 1.080596, 'test_loss': 280.954951, 'test_imp_ratio': -372.728304, 'test_acc': 0.0, 'test_total': 260, 'val_avg_loss': 0.340396, 'val_loss': 88.162557, 'val_imp_ratio': 37.943003, 'val_acc': 0.868726, 'val_total': 259}}
2022-09-03 16:47:00,537 (server:437) INFO: {'Role': 'Client #9', 'Round': 1, 'Results_raw': {'test_avg_loss': 1.982182, 'test_loss': 89003.948875, 'test_mse': 2.493122, 'test_imp_ratio': -4111.426442, 'test_total': 44902, 'val_avg_loss': 0.114289, 'val_loss': 5131.789083, 'val_mse': 0.143371, 'val_imp_ratio': -142.184181, 'val_total': 44902}}
2022-09-03 16:47:00,539 (server:437) INFO: {'Role': 'Client #10', 'Round': 1, 'Results_raw': {'test_avg_loss': 0.053352, 'test_loss': 1945.471632, 'test_mse': 0.067105, 'test_imp_ratio': -847.41301, 'test_total': 36465, 'val_avg_loss': 0.028845, 'val_loss': 1051.801159, 'val_mse': 0.036264, 'val_imp_ratio': -411.98569, 'val_total': 36464}}
2022-09-03 16:47:00,547 (server:437) INFO: {'Role': 'Client #11', 'Round': 1, 'Results_raw': {'test_avg_loss': 4.663154, 'test_loss': 3525.344225, 'test_mse': 5.865794, 'test_imp_ratio': -699.142479, 'test_total': 756, 'val_avg_loss': 1.115964, 'val_loss': 843.669054, 'val_mse': 1.355088, 'val_imp_ratio': -84.614175, 'val_total': 756}}
2022-09-03 16:47:00,549 (server:437) INFO: {'Role': 'Client #12', 'Round': 1, 'Results_raw': {'test_avg_loss': 9.363066, 'test_loss': 1900.70238, 'test_mse': 11.736158, 'test_imp_ratio': -762.112262, 'test_total': 203, 'val_avg_loss': 3.010603, 'val_loss': 611.152418, 'val_mse': 3.840567, 'val_imp_ratio': -182.119539, 'val_total': 203}}
2022-09-03 16:47:00,550 (server:437) INFO: {'Role': 'Client #13', 'Round': 1, 'Results_raw': {'test_avg_loss': 0.080019, 'test_loss': 1884.457657, 'test_mse': 0.100651, 'test_imp_ratio': -2193.258152, 'test_total': 23550, 'val_avg_loss': 0.038716, 'val_loss': 911.727312, 'val_mse': 0.04869, 'val_imp_ratio': -1009.365988, 'val_total': 23549}}
2022-09-03 16:47:00,561 (monitor:121) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 5.615334, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 530816, 'total_download_bytes': 25680, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2022-09-03 16:47:00,566 (client:440) INFO: ================= client 1 received finish message =================
